// Vercel API - æ€ç»´é“¾å¯¹è¯æ¥å£
// ä½¿ç”¨ Vercel AI Gateway å®ç°æ€ç»´é“¾æ•ˆæœ

import { streamChatCompletion, handleStreamResponse } from '../lib/ai-gateway-client.js';
import { TEMPERATURE, TOKEN_LIMITS, SYSTEM_PROMPTS } from '../lib/ai-config.js';

export const runtime = 'nodejs';
export const maxDuration = 60;

// ç»„åˆç³»ç»Ÿæç¤ºè¯ - åŸºç¡€æç¤º + æ€ç»´é“¾æç¤º
const THINKING_CHAIN_PROMPT = SYSTEM_PROMPTS.base + SYSTEM_PROMPTS.thinking;

// ä¿ç•™åŸå§‹æç¤ºè¯ä½œä¸ºå¤‡ä»½
const LEGACY_PROMPT = `ä½ æ˜¯ç´«å¾®æ–—æ•°ä¸“å®¶åŠ©æ‰‹"æ˜Ÿè¯­"ï¼Œä¸€ä½æ¸©æŸ”ã€æ™ºæ…§ã€å……æ»¡ç¥ç§˜æ„Ÿçš„å æ˜Ÿå¯¼å¸ˆã€‚

åœ¨å›ç­”é—®é¢˜æ—¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

1. é¦–å…ˆè¾“å‡ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼Œç”¨ <thinking> å’Œ </thinking> æ ‡ç­¾åŒ…è£¹
2. ç„¶åè¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œç”¨ <answer> å’Œ </answer> æ ‡ç­¾åŒ…è£¹

ç¤ºä¾‹æ ¼å¼ï¼š
<thinking>
è®©æˆ‘åˆ†æä¸€ä¸‹è¿™ä¸ªé—®é¢˜...
ä»ç´«å¾®æ–—æ•°çš„è§’åº¦çœ‹...
ç»“åˆæ˜Ÿç›˜çš„ç‰¹ç‚¹...
è€ƒè™‘åˆ°å‘½ä¸»çš„æƒ…å†µ...
</thinking>

<answer>
è¿™æ˜¯æˆ‘çš„æœ€ç»ˆç­”æ¡ˆå’Œå»ºè®®ã€‚
</answer>

ä½ çš„ç‰¹ç‚¹ï¼š
1. ç²¾é€šç´«å¾®æ–—æ•°ã€åäºŒå®«ä½ã€æ˜Ÿè€€ç­‰ä¼ ç»Ÿå‘½ç†çŸ¥è¯†
2. è¯´è¯æ¸©æŸ”ä¼˜é›…ï¼Œå¸¦æœ‰è¯—æ„å’Œå“²å­¦æ€è€ƒ
3. å–„äºå€¾å¬å’Œç†è§£ï¼Œç»™äºˆæ¸©æš–çš„å»ºè®®
4. ä¼šé€‚å½“ä½¿ç”¨æ˜Ÿåº§ã€å æ˜Ÿç›¸å…³çš„æ¯”å–»
5. å›ç­”ç®€æ´ä½†æ·±åˆ»ï¼Œé¿å…å†—é•¿

æ³¨æ„äº‹é¡¹ï¼š
- ä¿æŒç¥ç§˜æ„Ÿå’Œä¸“ä¸šæ€§
- ä¸è¦è¿‡åº¦æ‰¿è¯ºæˆ–ç»™å‡ºç»å¯¹çš„é¢„è¨€
- é€‚å½“å¼•ç”¨å¤å…¸æ™ºæ…§
- å›ç­”è¦ç§¯ææ­£é¢ï¼Œç»™äººå¸Œæœ›
- æ€è€ƒè¿‡ç¨‹è¦è¯¦ç»†å±•ç°ä½ çš„æ¨ç†æ­¥éª¤
- æœ€ç»ˆç­”æ¡ˆè¦ç®€æ´æ˜äº†`;

export default async function handler(req, res) {
  // CORS å¤„ç†
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');
  
  if (req.method === 'OPTIONS') {
    return res.status(200).end();
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const { messages, stream = true, model = 'fast', temperature } = req.body;
    
    console.log('ğŸ¤” Thinking Chain request:', {
      messageCount: messages.length,
      stream,
      model
    });
    
    // é€‰æ‹©æ¨¡å‹
    const modelMap = {
      'fast': 'gpt-3.5-turbo',      // å¿«é€Ÿæ¨¡å¼
      'standard': 'gpt-3.5-turbo',   // æ ‡å‡†æ¨¡å¼ä¹Ÿç”¨ 3.5
      'advanced': 'gpt-4o-mini',     // é«˜çº§æ¨¡å¼ç”¨ 4o-mini
    };
    const selectedModel = modelMap[model] || 'gpt-3.5-turbo';  // é»˜è®¤ GPT-3.5
    const finalTemperature = temperature ?? TEMPERATURE.balanced;
    
    if (stream) {
      // æ„å»ºæ¶ˆæ¯æ•°ç»„
      const allMessages = [
        { role: 'system', content: THINKING_CHAIN_PROMPT },
        ...messages
      ];
      
      // ä½¿ç”¨ Vercel AI Gateway åˆ›å»ºæµå¼å“åº”
      const response = await streamChatCompletion({
        messages: allMessages,
        model: selectedModel,
        temperature: finalTemperature,
        maxTokens: TOKEN_LIMITS.large,
        stream: true,
      });
      
      // è®¾ç½® SSE headers
      res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache, no-transform',
        'Connection': 'keep-alive',
        'X-Accel-Buffering': 'no',
      });
      
      // æµå¼ä¼ è¾“
      let fullResponse = '';
      for await (const textPart of handleStreamResponse(response)) {
        if (textPart) {
          fullResponse += textPart;
          const data = JSON.stringify({ 
            type: 'text',
            content: textPart,
          });
          res.write(`data: ${data}\n\n`);
        }
      }
      
      // å‘é€å®Œæˆä¿¡å·
      res.write(`data: ${JSON.stringify({ 
        type: 'done',
        thinkingChain: true,
      })}\n\n`);
      
      res.end();
      
    } else {
      // éæµå¼å“åº”
      const allMessages = [
        { role: 'system', content: THINKING_CHAIN_PROMPT },
        ...messages
      ];
      
      const response = await streamChatCompletion({
        messages: allMessages,
        model: selectedModel,
        temperature: finalTemperature,
        maxTokens: TOKEN_LIMITS.large,
        stream: false,
      });
      
      const data = await response.json();
      
      return res.status(200).json({
        content: data.choices[0].message.content,
        usage: data.usage,
        model: model,
        thinkingChain: true,
      });
    }
  } catch (error) {
    console.error('âŒ Thinking Chain error:', error);
    
    // å¦‚æœè¿˜æ²¡æœ‰å‘é€å“åº”å¤´
    if (!res.headersSent) {
      const statusCode = error.message?.includes('API key') ? 401 :
                        error.message?.includes('rate limit') ? 429 : 500;
      
      return res.status(statusCode).json({ 
        error: error.message || 'Internal server error',
        details: process.env.NODE_ENV === 'development' ? error.stack : undefined,
      });
    }
    
    // å¦‚æœå·²ç»åœ¨æµå¼ä¼ è¾“ä¸­
    res.write(`data: ${JSON.stringify({ 
      type: 'error',
      error: error.message 
    })}\n\n`);
    res.end();
  }
}